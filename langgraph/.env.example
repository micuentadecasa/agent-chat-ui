# LM Studio Configuration
# Base URL for LM Studio server (default port is 1234)
LM_STUDIO_BASE_URL=http://localhost:1234/v1

# API key (LM Studio doesn't require authentication, but some tools expect a value)
LM_STUDIO_API_KEY=lm-studio

# Model name - IMPORTANT: Replace with your actual loaded model name
# Examples: llama-3.1-8b-instruct, mistral-7b-instruct-v0.2, qwen2.5-7b-instruct
# To find your model name: Check LM Studio's "My Models" tab or the server logs
LM_STUDIO_MODEL=llama-3.1-8b-instruct

# LLM Generation Parameters (used by both BAML and LangGraph)
LM_STUDIO_TEMPERATURE=0.7
LM_STUDIO_MAX_TOKENS=1000

# LangGraph Development Configuration
LANGGRAPH_DEV_PORT=2024

# LangGraph Settings
THREAD_TIMEOUT=300
MAX_ITERATIONS=10

# BAML Configuration
BAML_ENV=dev

# Logging Configuration
LOG_LEVEL=INFO
LOG_TO_FILE=true
LOG_FILE=logs/langgraph.log
LOG_JSON_FORMAT=false
DEBUG_MODE=false